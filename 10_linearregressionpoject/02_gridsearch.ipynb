{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "      TV  radio  newspaper  sales\n0  230.1   37.8       69.2   22.1\n1   44.5   39.3       45.1   10.4\n2   17.2   45.9       69.3    9.3\n3  151.5   41.3       58.5   18.5\n4  180.8   10.8       58.4   12.9",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TV</th>\n      <th>radio</th>\n      <th>newspaper</th>\n      <th>sales</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>230.1</td>\n      <td>37.8</td>\n      <td>69.2</td>\n      <td>22.1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>44.5</td>\n      <td>39.3</td>\n      <td>45.1</td>\n      <td>10.4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>17.2</td>\n      <td>45.9</td>\n      <td>69.3</td>\n      <td>9.3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>151.5</td>\n      <td>41.3</td>\n      <td>58.5</td>\n      <td>18.5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>180.8</td>\n      <td>10.8</td>\n      <td>58.4</td>\n      <td>12.9</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('../course_notebooks/DATA/Advertising.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "x = df.drop('sales', axis=1)\n",
    "y = df['sales']\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=101)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ElasticNet in module sklearn.linear_model._coordinate_descent:\n",
      "\n",
      "class ElasticNet(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, sklearn.linear_model._base.LinearModel)\n",
      " |  ElasticNet(alpha=1.0, *, l1_ratio=0.5, fit_intercept=True, normalize='deprecated', precompute=False, max_iter=1000, copy_X=True, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
      " |  \n",
      " |  Linear regression with combined L1 and L2 priors as regularizer.\n",
      " |  \n",
      " |  Minimizes the objective function::\n",
      " |  \n",
      " |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      " |          + alpha * l1_ratio * ||w||_1\n",
      " |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      " |  \n",
      " |  If you are interested in controlling the L1 and L2 penalty\n",
      " |  separately, keep in mind that this is equivalent to::\n",
      " |  \n",
      " |          a * ||w||_1 + 0.5 * b * ||w||_2^2\n",
      " |  \n",
      " |  where::\n",
      " |  \n",
      " |          alpha = a + b and l1_ratio = a / (a + b)\n",
      " |  \n",
      " |  The parameter l1_ratio corresponds to alpha in the glmnet R package while\n",
      " |  alpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio\n",
      " |  = 1 is the lasso penalty. Currently, l1_ratio <= 0.01 is not reliable,\n",
      " |  unless you supply your own sequence of alpha.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <elastic_net>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  alpha : float, default=1.0\n",
      " |      Constant that multiplies the penalty terms. Defaults to 1.0.\n",
      " |      See the notes for the exact mathematical meaning of this\n",
      " |      parameter. ``alpha = 0`` is equivalent to an ordinary least square,\n",
      " |      solved by the :class:`LinearRegression` object. For numerical\n",
      " |      reasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.\n",
      " |      Given this, you should use the :class:`LinearRegression` object.\n",
      " |  \n",
      " |  l1_ratio : float, default=0.5\n",
      " |      The ElasticNet mixing parameter, with ``0 <= l1_ratio <= 1``. For\n",
      " |      ``l1_ratio = 0`` the penalty is an L2 penalty. ``For l1_ratio = 1`` it\n",
      " |      is an L1 penalty.  For ``0 < l1_ratio < 1``, the penalty is a\n",
      " |      combination of L1 and L2.\n",
      " |  \n",
      " |  fit_intercept : bool, default=True\n",
      " |      Whether the intercept should be estimated or not. If ``False``, the\n",
      " |      data is assumed to be already centered.\n",
      " |  \n",
      " |  normalize : bool, default=False\n",
      " |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      " |      If True, the regressors X will be normalized before regression by\n",
      " |      subtracting the mean and dividing by the l2-norm.\n",
      " |      If you wish to standardize, please use\n",
      " |      :class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``\n",
      " |      on an estimator with ``normalize=False``.\n",
      " |  \n",
      " |      .. deprecated:: 1.0\n",
      " |          ``normalize`` was deprecated in version 1.0 and will be removed in\n",
      " |          1.2.\n",
      " |  \n",
      " |  precompute : bool or array-like of shape (n_features, n_features),                 default=False\n",
      " |      Whether to use a precomputed Gram matrix to speed up\n",
      " |      calculations. The Gram matrix can also be passed as argument.\n",
      " |      For sparse input this option is always ``False`` to preserve sparsity.\n",
      " |  \n",
      " |  max_iter : int, default=1000\n",
      " |      The maximum number of iterations.\n",
      " |  \n",
      " |  copy_X : bool, default=True\n",
      " |      If ``True``, X will be copied; else, it may be overwritten.\n",
      " |  \n",
      " |  tol : float, default=1e-4\n",
      " |      The tolerance for the optimization: if the updates are\n",
      " |      smaller than ``tol``, the optimization code checks the\n",
      " |      dual gap for optimality and continues until it is smaller\n",
      " |      than ``tol``.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit as\n",
      " |      initialization, otherwise, just erase the previous solution.\n",
      " |      See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |  positive : bool, default=False\n",
      " |      When set to ``True``, forces the coefficients to be positive.\n",
      " |  \n",
      " |  random_state : int, RandomState instance, default=None\n",
      " |      The seed of the pseudo random number generator that selects a random\n",
      " |      feature to update. Used when ``selection`` == 'random'.\n",
      " |      Pass an int for reproducible output across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |  selection : {'cyclic', 'random'}, default='cyclic'\n",
      " |      If set to 'random', a random coefficient is updated every iteration\n",
      " |      rather than looping over features sequentially by default. This\n",
      " |      (setting to 'random') often leads to significantly faster convergence\n",
      " |      especially when tol is higher than 1e-4.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  coef_ : ndarray of shape (n_features,) or (n_targets, n_features)\n",
      " |      Parameter vector (w in the cost function formula).\n",
      " |  \n",
      " |  sparse_coef_ : sparse matrix of shape (n_features,) or             (n_targets, n_features)\n",
      " |      Sparse representation of the `coef_`.\n",
      " |  \n",
      " |  intercept_ : float or ndarray of shape (n_targets,)\n",
      " |      Independent term in decision function.\n",
      " |  \n",
      " |  n_iter_ : list of int\n",
      " |      Number of iterations run by the coordinate descent solver to reach\n",
      " |      the specified tolerance.\n",
      " |  \n",
      " |  dual_gap_ : float or ndarray of shape (n_targets,)\n",
      " |      Given param alpha, the dual gaps at the end of the optimization,\n",
      " |      same shape as each observation of y.\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  ElasticNetCV : Elastic net model with best model selection by\n",
      " |      cross-validation.\n",
      " |  SGDRegressor : Implements elastic net regression with incremental training.\n",
      " |  SGDClassifier : Implements logistic regression with elastic net penalty\n",
      " |      (``SGDClassifier(loss=\"log\", penalty=\"elasticnet\")``).\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  To avoid unnecessary memory duplication the X argument of the fit method\n",
      " |  should be directly passed as a Fortran-contiguous numpy array.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.linear_model import ElasticNet\n",
      " |  >>> from sklearn.datasets import make_regression\n",
      " |  \n",
      " |  >>> X, y = make_regression(n_features=2, random_state=0)\n",
      " |  >>> regr = ElasticNet(random_state=0)\n",
      " |  >>> regr.fit(X, y)\n",
      " |  ElasticNet(random_state=0)\n",
      " |  >>> print(regr.coef_)\n",
      " |  [18.83816048 64.55968825]\n",
      " |  >>> print(regr.intercept_)\n",
      " |  1.451...\n",
      " |  >>> print(regr.predict([[0, 0]]))\n",
      " |  [1.451...]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ElasticNet\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      sklearn.linear_model._base.LinearModel\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, alpha=1.0, *, l1_ratio=0.5, fit_intercept=True, normalize='deprecated', precompute=False, max_iter=1000, copy_X=True, tol=0.0001, warm_start=False, positive=False, random_state=None, selection='cyclic')\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None, check_input=True)\n",
      " |      Fit model with coordinate descent.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {ndarray, sparse matrix} of (n_samples, n_features)\n",
      " |          Data.\n",
      " |      \n",
      " |      y : {ndarray, sparse matrix} of shape (n_samples,) or             (n_samples, n_targets)\n",
      " |          Target. Will be cast to X's dtype if necessary.\n",
      " |      \n",
      " |      sample_weight : float or array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. Internally, the `sample_weight` vector will be\n",
      " |          rescaled to sum to `n_samples`.\n",
      " |      \n",
      " |          .. versionadded:: 0.23\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          Allow to bypass several input checking.\n",
      " |          Don't use this parameter unless you know what you do.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted estimator.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Coordinate descent is an algorithm that considers each column of\n",
      " |      data at a time hence it will automatically convert the X input\n",
      " |      as a Fortran-contiguous numpy array if necessary.\n",
      " |      \n",
      " |      To avoid memory re-allocation it is advised to allocate the\n",
      " |      initial data in memory directly using that format.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  path = enet_path(X, y, *, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute='auto', Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)\n",
      " |      Compute elastic net path with coordinate descent.\n",
      " |      \n",
      " |      The elastic net optimization function varies for mono and multi-outputs.\n",
      " |      \n",
      " |      For mono-output tasks it is::\n",
      " |      \n",
      " |          1 / (2 * n_samples) * ||y - Xw||^2_2\n",
      " |          + alpha * l1_ratio * ||w||_1\n",
      " |          + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2\n",
      " |      \n",
      " |      For multi-output tasks it is::\n",
      " |      \n",
      " |          (1 / (2 * n_samples)) * ||Y - XW||_Fro^2\n",
      " |          + alpha * l1_ratio * ||W||_21\n",
      " |          + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2\n",
      " |      \n",
      " |      Where::\n",
      " |      \n",
      " |          ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}\n",
      " |      \n",
      " |      i.e. the sum of norm of each row.\n",
      " |      \n",
      " |      Read more in the :ref:`User Guide <elastic_net>`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training data. Pass directly as Fortran-contiguous data to avoid\n",
      " |          unnecessary memory duplication. If ``y`` is mono-output then ``X``\n",
      " |          can be sparse.\n",
      " |      \n",
      " |      y : {array-like, sparse matrix} of shape (n_samples,) or         (n_samples, n_targets)\n",
      " |          Target values.\n",
      " |      \n",
      " |      l1_ratio : float, default=0.5\n",
      " |          Number between 0 and 1 passed to elastic net (scaling between\n",
      " |          l1 and l2 penalties). ``l1_ratio=1`` corresponds to the Lasso.\n",
      " |      \n",
      " |      eps : float, default=1e-3\n",
      " |          Length of the path. ``eps=1e-3`` means that\n",
      " |          ``alpha_min / alpha_max = 1e-3``.\n",
      " |      \n",
      " |      n_alphas : int, default=100\n",
      " |          Number of alphas along the regularization path.\n",
      " |      \n",
      " |      alphas : ndarray, default=None\n",
      " |          List of alphas where to compute the models.\n",
      " |          If None alphas are set automatically.\n",
      " |      \n",
      " |      precompute : 'auto', bool or array-like of shape             (n_features, n_features), default='auto'\n",
      " |          Whether to use a precomputed Gram matrix to speed up\n",
      " |          calculations. If set to ``'auto'`` let us decide. The Gram\n",
      " |          matrix can also be passed as argument.\n",
      " |      \n",
      " |      Xy : array-like of shape (n_features,) or (n_features, n_targets),         default=None\n",
      " |          Xy = np.dot(X.T, y) that can be precomputed. It is useful\n",
      " |          only when the Gram matrix is precomputed.\n",
      " |      \n",
      " |      copy_X : bool, default=True\n",
      " |          If ``True``, X will be copied; else, it may be overwritten.\n",
      " |      \n",
      " |      coef_init : ndarray of shape (n_features, ), default=None\n",
      " |          The initial values of the coefficients.\n",
      " |      \n",
      " |      verbose : bool or int, default=False\n",
      " |          Amount of verbosity.\n",
      " |      \n",
      " |      return_n_iter : bool, default=False\n",
      " |          Whether to return the number of iterations or not.\n",
      " |      \n",
      " |      positive : bool, default=False\n",
      " |          If set to True, forces coefficients to be positive.\n",
      " |          (Only allowed when ``y.ndim == 1``).\n",
      " |      \n",
      " |      check_input : bool, default=True\n",
      " |          If set to False, the input validation checks are skipped (including the\n",
      " |          Gram matrix when provided). It is assumed that they are handled\n",
      " |          by the caller.\n",
      " |      \n",
      " |      **params : kwargs\n",
      " |          Keyword arguments passed to the coordinate descent solver.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      alphas : ndarray of shape (n_alphas,)\n",
      " |          The alphas along the path where models are computed.\n",
      " |      \n",
      " |      coefs : ndarray of shape (n_features, n_alphas) or             (n_targets, n_features, n_alphas)\n",
      " |          Coefficients along the path.\n",
      " |      \n",
      " |      dual_gaps : ndarray of shape (n_alphas,)\n",
      " |          The dual gaps at the end of the optimization for each alpha.\n",
      " |      \n",
      " |      n_iters : list of int\n",
      " |          The number of iterations taken by the coordinate descent optimizer to\n",
      " |          reach the specified tolerance for each alpha.\n",
      " |          (Is returned when ``return_n_iter`` is set to True).\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      MultiTaskElasticNet : Multi-task ElasticNet model trained with L1/L2 mixed-norm     as regularizer.\n",
      " |      MultiTaskElasticNetCV : Multi-task L1/L2 ElasticNet with built-in cross-validation.\n",
      " |      ElasticNet : Linear regression with combined L1 and L2 priors as regularizer.\n",
      " |      ElasticNetCV : Elastic Net model with iterative fitting along a regularization path.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For an example, see\n",
      " |      :ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py\n",
      " |      <sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  sparse_coef_\n",
      " |      Sparse representation of the fitted `coef_`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the coefficient of determination of the prediction.\n",
      " |      \n",
      " |      The coefficient of determination :math:`R^2` is defined as\n",
      " |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      " |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      " |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always predicts\n",
      " |      the expected value of `y`, disregarding the input features, would get\n",
      " |      a :math:`R^2` score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples. For some estimators this may be a precomputed\n",
      " |          kernel matrix or a list of generic objects instead with shape\n",
      " |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      " |          is the number of samples used in the fitting for the estimator.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True values for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      " |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      " |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      " |      This influences the ``score`` method of all the multioutput\n",
      " |      regressors (except for\n",
      " |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.LinearModel:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict using the linear model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape (n_samples,)\n",
      " |          Returns predicted values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "help(ElasticNet)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 42 candidates, totalling 210 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Dev\\python\\course\\venv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.784e+02, tolerance: 2.817e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "F:\\Dev\\python\\course\\venv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.048e+02, tolerance: 3.053e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "F:\\Dev\\python\\course\\venv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.455e+02, tolerance: 2.831e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "F:\\Dev\\python\\course\\venv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.933e+02, tolerance: 2.905e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "F:\\Dev\\python\\course\\venv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.660e+02, tolerance: 3.095e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "F:\\Dev\\python\\course\\venv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.997e+02, tolerance: 2.905e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "F:\\Dev\\python\\course\\venv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.281e+03, tolerance: 3.053e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "F:\\Dev\\python\\course\\venv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.178e+03, tolerance: 2.831e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "F:\\Dev\\python\\course\\venv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.391e+03, tolerance: 3.053e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "F:\\Dev\\python\\course\\venv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.497e+03, tolerance: 3.053e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "F:\\Dev\\python\\course\\venv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.387e+03, tolerance: 2.831e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "F:\\Dev\\python\\course\\venv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.426e+03, tolerance: 2.905e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "F:\\Dev\\python\\course\\venv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.514e+03, tolerance: 3.095e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "F:\\Dev\\python\\course\\venv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.395e+03, tolerance: 2.817e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "F:\\Dev\\python\\course\\venv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.511e+03, tolerance: 3.053e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "F:\\Dev\\python\\course\\venv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.401e+03, tolerance: 2.831e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "F:\\Dev\\python\\course\\venv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.439e+03, tolerance: 2.905e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
      "F:\\Dev\\python\\course\\venv\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.530e+03, tolerance: 3.095e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
     ]
    },
    {
     "data": {
      "text/plain": "GridSearchCV(cv=5, estimator=ElasticNet(),\n             param_grid={'alpha': [0.1, 1, 5, 10, 50, 100],\n                         'l1_ratio': [0, 0.25, 0.5, 0.75, 0.95, 0.99, 1]},\n             scoring='neg_mean_squared_error', verbose=1)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_elastic_net_model = ElasticNet()\n",
    "param_grid = {'alpha':[0.1,1,5,10,50,100],'l1_ratio':[0,0.25,0.5,0.75,0.95,0.99,1]}\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_model = GridSearchCV(base_elastic_net_model,param_grid,scoring='neg_mean_squared_error',cv=5,verbose=1)\n",
    "grid_model.fit(x_train,y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet(alpha=0.1, l1_ratio=1)\n",
      "{'alpha': 0.1, 'l1_ratio': 1}\n",
      "-3.2927601581001076\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(grid_model.best_estimator_)\n",
    "print(grid_model.best_params_)\n",
    "print(grid_model.best_score_)\n",
    "print(grid_model.best_index_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_alpha  \\\n0        0.001995  9.655217e-07         0.000399        0.000489         0.1   \n1        0.000997  8.714517e-07         0.000161        0.000322         0.1   \n2        0.001196  3.989946e-04         0.000199        0.000399         0.1   \n3        0.002194  1.934325e-03         0.000398        0.000488         0.1   \n4        0.000997  3.162980e-07         0.000199        0.000399         0.1   \n5        0.000910  5.036837e-04         0.000200        0.000399         0.1   \n6        0.000798  3.988986e-04         0.000200        0.000399         0.1   \n7        0.000997  6.308267e-04         0.000200        0.000399           1   \n8        0.000562  3.971106e-04         0.000199        0.000399           1   \n9        0.000798  3.989460e-04         0.000199        0.000399           1   \n10       0.001397  4.884228e-04         0.000000        0.000000           1   \n11       0.000797  3.987314e-04         0.000199        0.000399           1   \n12       0.000597  4.876111e-04         0.000399        0.000488           1   \n13       0.000598  4.884220e-04         0.000548        0.000457           1   \n14       0.000798  3.989460e-04         0.000598        0.000488           5   \n15       0.000798  3.988505e-04         0.000200        0.000399           5   \n16       0.000598  4.884999e-04         0.000546        0.000456           5   \n17       0.000599  4.888115e-04         0.000399        0.000489           5   \n18       0.000798  3.988029e-04         0.000199        0.000399           5   \n19       0.000997  2.861023e-07         0.000200        0.000399           5   \n20       0.000798  3.989937e-04         0.000399        0.000488           5   \n21       0.001197  3.986603e-04         0.000200        0.000399          10   \n22       0.000798  3.989698e-04         0.000399        0.000489          10   \n23       0.001090  1.852764e-04         0.000599        0.000798          10   \n24       0.001197  3.992086e-04         0.000200        0.000399          10   \n25       0.001390  4.817802e-04         0.000598        0.000489          10   \n26       0.001396  4.883055e-04         0.000399        0.000489          10   \n27       0.000798  3.989936e-04         0.000399        0.000489          10   \n28       0.003391  2.053896e-03         0.000399        0.000488          50   \n29       0.000997  8.920806e-07         0.000598        0.000489          50   \n30       0.001196  7.459202e-04         0.000199        0.000399          50   \n31       0.000599  4.887729e-04         0.000399        0.000488          50   \n32       0.002593  1.196885e-03         0.000798        0.000746          50   \n33       0.001197  3.988272e-04         0.000399        0.000488          50   \n34       0.001197  3.990889e-04         0.000199        0.000399          50   \n35       0.001397  4.884220e-04         0.000598        0.000488         100   \n36       0.000840  3.154518e-04         0.000200        0.000399         100   \n37       0.000998  3.568323e-07         0.000199        0.000399         100   \n38       0.000598  4.884609e-04         0.000399        0.000489         100   \n39       0.000798  3.988507e-04         0.000599        0.000799         100   \n40       0.001247  9.719796e-04         0.000398        0.000488         100   \n41       0.000886  4.758855e-04         0.000200        0.000399         100   \n\n   param_l1_ratio                            params  split0_test_score  \\\n0               0     {'alpha': 0.1, 'l1_ratio': 0}          -3.487712   \n1            0.25  {'alpha': 0.1, 'l1_ratio': 0.25}          -3.402924   \n2             0.5   {'alpha': 0.1, 'l1_ratio': 0.5}          -3.325440   \n3            0.75  {'alpha': 0.1, 'l1_ratio': 0.75}          -3.257050   \n4            0.95  {'alpha': 0.1, 'l1_ratio': 0.95}          -3.213052   \n5            0.99  {'alpha': 0.1, 'l1_ratio': 0.99}          -3.208124   \n6               1     {'alpha': 0.1, 'l1_ratio': 1}          -3.206943   \n7               0       {'alpha': 1, 'l1_ratio': 0}         -10.051972   \n8            0.25    {'alpha': 1, 'l1_ratio': 0.25}          -9.452682   \n9             0.5     {'alpha': 1, 'l1_ratio': 0.5}          -8.707071   \n10           0.75    {'alpha': 1, 'l1_ratio': 0.75}          -7.699543   \n11           0.95    {'alpha': 1, 'l1_ratio': 0.95}          -6.729435   \n12           0.99    {'alpha': 1, 'l1_ratio': 0.99}          -6.521344   \n13              1       {'alpha': 1, 'l1_ratio': 1}          -6.468807   \n14              0       {'alpha': 5, 'l1_ratio': 0}         -22.172571   \n15           0.25    {'alpha': 5, 'l1_ratio': 0.25}         -24.432713   \n16            0.5     {'alpha': 5, 'l1_ratio': 0.5}         -27.793488   \n17           0.75    {'alpha': 5, 'l1_ratio': 0.75}         -30.329236   \n18           0.95    {'alpha': 5, 'l1_ratio': 0.95}         -31.130307   \n19           0.99    {'alpha': 5, 'l1_ratio': 0.99}         -31.130307   \n20              1       {'alpha': 5, 'l1_ratio': 1}         -31.130307   \n21              0      {'alpha': 10, 'l1_ratio': 0}         -25.971218   \n22           0.25   {'alpha': 10, 'l1_ratio': 0.25}         -29.700286   \n23            0.5    {'alpha': 10, 'l1_ratio': 0.5}         -31.130307   \n24           0.75   {'alpha': 10, 'l1_ratio': 0.75}         -31.130307   \n25           0.95   {'alpha': 10, 'l1_ratio': 0.95}         -31.130307   \n26           0.99   {'alpha': 10, 'l1_ratio': 0.99}         -31.130307   \n27              1      {'alpha': 10, 'l1_ratio': 1}         -31.130307   \n28              0      {'alpha': 50, 'l1_ratio': 0}         -29.959161   \n29           0.25   {'alpha': 50, 'l1_ratio': 0.25}         -31.130307   \n30            0.5    {'alpha': 50, 'l1_ratio': 0.5}         -31.130307   \n31           0.75   {'alpha': 50, 'l1_ratio': 0.75}         -31.130307   \n32           0.95   {'alpha': 50, 'l1_ratio': 0.95}         -31.130307   \n33           0.99   {'alpha': 50, 'l1_ratio': 0.99}         -31.130307   \n34              1      {'alpha': 50, 'l1_ratio': 1}         -31.130307   \n35              0     {'alpha': 100, 'l1_ratio': 0}         -30.534802   \n36           0.25  {'alpha': 100, 'l1_ratio': 0.25}         -31.130307   \n37            0.5   {'alpha': 100, 'l1_ratio': 0.5}         -31.130307   \n38           0.75  {'alpha': 100, 'l1_ratio': 0.75}         -31.130307   \n39           0.95  {'alpha': 100, 'l1_ratio': 0.95}         -31.130307   \n40           0.99  {'alpha': 100, 'l1_ratio': 0.99}         -31.130307   \n41              1     {'alpha': 100, 'l1_ratio': 1}         -31.130307   \n\n    split1_test_score  split2_test_score  split3_test_score  \\\n0           -1.401773          -5.838736          -2.198521   \n1           -1.415007          -5.715566          -2.174133   \n2           -1.427522          -5.595610          -2.163089   \n3           -1.447376          -5.479585          -2.165427   \n4           -1.472417          -5.396258          -2.177452   \n5           -1.478489          -5.380242          -2.181097   \n6           -1.480065          -5.376257          -2.182076   \n7           -5.484766         -12.057264          -7.700462   \n8           -4.897429         -11.576465          -7.030995   \n9           -4.214228         -10.879261          -6.204545   \n10          -3.367100          -9.785429          -5.150364   \n11          -2.591285          -8.709842          -4.156317   \n12          -2.431385          -8.471086          -3.946327   \n13          -2.391483          -8.410171          -3.893566   \n14         -15.069784         -22.993944         -19.282304   \n15         -16.467589         -24.765866         -21.125351   \n16         -18.602269         -27.107849         -23.945227   \n17         -21.745310         -30.310118         -27.557643   \n18         -22.549433         -31.155204         -27.963447   \n19         -22.549433         -31.155204         -27.963447   \n20         -22.549433         -31.155204         -27.963447   \n21         -18.208243         -26.439339         -22.955705   \n22         -20.857661         -29.415492         -26.258753   \n23         -22.549433         -31.155204         -27.963447   \n24         -22.549433         -31.155204         -27.963447   \n25         -22.549433         -31.155204         -27.963447   \n26         -22.549433         -31.155204         -27.963447   \n27         -22.549433         -31.155204         -27.963447   \n28         -21.556223         -30.080478         -26.824729   \n29         -22.549433         -31.155204         -27.963447   \n30         -22.549433         -31.155204         -27.963447   \n31         -22.549433         -31.155204         -27.963447   \n32         -22.549433         -31.155204         -27.963447   \n33         -22.549433         -31.155204         -27.963447   \n34         -22.549433         -31.155204         -27.963447   \n35         -22.043843         -30.608400         -27.384285   \n36         -22.549433         -31.155204         -27.963447   \n37         -22.549433         -31.155204         -27.963447   \n38         -22.549433         -31.155204         -27.963447   \n39         -22.549433         -31.155204         -27.963447   \n40         -22.549433         -31.155204         -27.963447   \n41         -22.549433         -31.155204         -27.963447   \n\n    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n0           -4.694513        -3.524251        1.611841                7  \n1           -4.572409        -3.456008        1.559722                6  \n2           -4.451679        -3.392668        1.506827                5  \n3           -4.333304        -3.336548        1.451409                4  \n4           -4.241080        -3.300052        1.406248                3  \n5           -4.222968        -3.294184        1.396953                2  \n6           -4.218460        -3.292760        1.394613                1  \n7           -8.815054        -8.821904        2.207931               14  \n8           -8.085862        -8.208687        2.247484               13  \n9           -7.173031        -7.435627        2.255532               12  \n10          -6.142527        -6.428993        2.189617               11  \n11          -5.329916        -5.503359        2.102835               10  \n12          -5.151344        -5.304297        2.079945                9  \n13          -5.105922        -5.253990        2.073832                8  \n14         -16.044044       -19.112529        3.169147               15  \n15         -16.714276       -20.701159        3.590115               16  \n16         -18.064635       -23.102694        4.108297               18  \n17         -21.013430       -26.191148        4.062788               21  \n18         -21.698192       -26.899317        4.077240               23  \n19         -21.698192       -26.899317        4.077240               23  \n20         -21.698192       -26.899317        4.077240               23  \n21         -18.400074       -22.394916        3.548205               17  \n22         -20.130788       -25.272596        4.090730               19  \n23         -21.698192       -26.899317        4.077240               23  \n24         -21.698192       -26.899317        4.077240               23  \n25         -21.698192       -26.899317        4.077240               23  \n26         -21.698192       -26.899317        4.077240               23  \n27         -21.698192       -26.899317        4.077240               23  \n28         -20.938967       -25.871912        3.956883               20  \n29         -21.698192       -26.899317        4.077240               23  \n30         -21.698192       -26.899317        4.077240               23  \n31         -21.698192       -26.899317        4.077240               23  \n32         -21.698192       -26.899317        4.077240               23  \n33         -21.698192       -26.899317        4.077240               23  \n34         -21.698192       -26.899317        4.077240               23  \n35         -21.311346       -26.376535        4.016062               22  \n36         -21.698192       -26.899317        4.077240               23  \n37         -21.698192       -26.899317        4.077240               23  \n38         -21.698192       -26.899317        4.077240               23  \n39         -21.698192       -26.899317        4.077240               23  \n40         -21.698192       -26.899317        4.077240               23  \n41         -21.698192       -26.899317        4.077240               23  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_alpha</th>\n      <th>param_l1_ratio</th>\n      <th>params</th>\n      <th>split0_test_score</th>\n      <th>split1_test_score</th>\n      <th>split2_test_score</th>\n      <th>split3_test_score</th>\n      <th>split4_test_score</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>rank_test_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.001995</td>\n      <td>9.655217e-07</td>\n      <td>0.000399</td>\n      <td>0.000489</td>\n      <td>0.1</td>\n      <td>0</td>\n      <td>{'alpha': 0.1, 'l1_ratio': 0}</td>\n      <td>-3.487712</td>\n      <td>-1.401773</td>\n      <td>-5.838736</td>\n      <td>-2.198521</td>\n      <td>-4.694513</td>\n      <td>-3.524251</td>\n      <td>1.611841</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.000997</td>\n      <td>8.714517e-07</td>\n      <td>0.000161</td>\n      <td>0.000322</td>\n      <td>0.1</td>\n      <td>0.25</td>\n      <td>{'alpha': 0.1, 'l1_ratio': 0.25}</td>\n      <td>-3.402924</td>\n      <td>-1.415007</td>\n      <td>-5.715566</td>\n      <td>-2.174133</td>\n      <td>-4.572409</td>\n      <td>-3.456008</td>\n      <td>1.559722</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.001196</td>\n      <td>3.989946e-04</td>\n      <td>0.000199</td>\n      <td>0.000399</td>\n      <td>0.1</td>\n      <td>0.5</td>\n      <td>{'alpha': 0.1, 'l1_ratio': 0.5}</td>\n      <td>-3.325440</td>\n      <td>-1.427522</td>\n      <td>-5.595610</td>\n      <td>-2.163089</td>\n      <td>-4.451679</td>\n      <td>-3.392668</td>\n      <td>1.506827</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.002194</td>\n      <td>1.934325e-03</td>\n      <td>0.000398</td>\n      <td>0.000488</td>\n      <td>0.1</td>\n      <td>0.75</td>\n      <td>{'alpha': 0.1, 'l1_ratio': 0.75}</td>\n      <td>-3.257050</td>\n      <td>-1.447376</td>\n      <td>-5.479585</td>\n      <td>-2.165427</td>\n      <td>-4.333304</td>\n      <td>-3.336548</td>\n      <td>1.451409</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000997</td>\n      <td>3.162980e-07</td>\n      <td>0.000199</td>\n      <td>0.000399</td>\n      <td>0.1</td>\n      <td>0.95</td>\n      <td>{'alpha': 0.1, 'l1_ratio': 0.95}</td>\n      <td>-3.213052</td>\n      <td>-1.472417</td>\n      <td>-5.396258</td>\n      <td>-2.177452</td>\n      <td>-4.241080</td>\n      <td>-3.300052</td>\n      <td>1.406248</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.000910</td>\n      <td>5.036837e-04</td>\n      <td>0.000200</td>\n      <td>0.000399</td>\n      <td>0.1</td>\n      <td>0.99</td>\n      <td>{'alpha': 0.1, 'l1_ratio': 0.99}</td>\n      <td>-3.208124</td>\n      <td>-1.478489</td>\n      <td>-5.380242</td>\n      <td>-2.181097</td>\n      <td>-4.222968</td>\n      <td>-3.294184</td>\n      <td>1.396953</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.000798</td>\n      <td>3.988986e-04</td>\n      <td>0.000200</td>\n      <td>0.000399</td>\n      <td>0.1</td>\n      <td>1</td>\n      <td>{'alpha': 0.1, 'l1_ratio': 1}</td>\n      <td>-3.206943</td>\n      <td>-1.480065</td>\n      <td>-5.376257</td>\n      <td>-2.182076</td>\n      <td>-4.218460</td>\n      <td>-3.292760</td>\n      <td>1.394613</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.000997</td>\n      <td>6.308267e-04</td>\n      <td>0.000200</td>\n      <td>0.000399</td>\n      <td>1</td>\n      <td>0</td>\n      <td>{'alpha': 1, 'l1_ratio': 0}</td>\n      <td>-10.051972</td>\n      <td>-5.484766</td>\n      <td>-12.057264</td>\n      <td>-7.700462</td>\n      <td>-8.815054</td>\n      <td>-8.821904</td>\n      <td>2.207931</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.000562</td>\n      <td>3.971106e-04</td>\n      <td>0.000199</td>\n      <td>0.000399</td>\n      <td>1</td>\n      <td>0.25</td>\n      <td>{'alpha': 1, 'l1_ratio': 0.25}</td>\n      <td>-9.452682</td>\n      <td>-4.897429</td>\n      <td>-11.576465</td>\n      <td>-7.030995</td>\n      <td>-8.085862</td>\n      <td>-8.208687</td>\n      <td>2.247484</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.000798</td>\n      <td>3.989460e-04</td>\n      <td>0.000199</td>\n      <td>0.000399</td>\n      <td>1</td>\n      <td>0.5</td>\n      <td>{'alpha': 1, 'l1_ratio': 0.5}</td>\n      <td>-8.707071</td>\n      <td>-4.214228</td>\n      <td>-10.879261</td>\n      <td>-6.204545</td>\n      <td>-7.173031</td>\n      <td>-7.435627</td>\n      <td>2.255532</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.001397</td>\n      <td>4.884228e-04</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1</td>\n      <td>0.75</td>\n      <td>{'alpha': 1, 'l1_ratio': 0.75}</td>\n      <td>-7.699543</td>\n      <td>-3.367100</td>\n      <td>-9.785429</td>\n      <td>-5.150364</td>\n      <td>-6.142527</td>\n      <td>-6.428993</td>\n      <td>2.189617</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.000797</td>\n      <td>3.987314e-04</td>\n      <td>0.000199</td>\n      <td>0.000399</td>\n      <td>1</td>\n      <td>0.95</td>\n      <td>{'alpha': 1, 'l1_ratio': 0.95}</td>\n      <td>-6.729435</td>\n      <td>-2.591285</td>\n      <td>-8.709842</td>\n      <td>-4.156317</td>\n      <td>-5.329916</td>\n      <td>-5.503359</td>\n      <td>2.102835</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0.000597</td>\n      <td>4.876111e-04</td>\n      <td>0.000399</td>\n      <td>0.000488</td>\n      <td>1</td>\n      <td>0.99</td>\n      <td>{'alpha': 1, 'l1_ratio': 0.99}</td>\n      <td>-6.521344</td>\n      <td>-2.431385</td>\n      <td>-8.471086</td>\n      <td>-3.946327</td>\n      <td>-5.151344</td>\n      <td>-5.304297</td>\n      <td>2.079945</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0.000598</td>\n      <td>4.884220e-04</td>\n      <td>0.000548</td>\n      <td>0.000457</td>\n      <td>1</td>\n      <td>1</td>\n      <td>{'alpha': 1, 'l1_ratio': 1}</td>\n      <td>-6.468807</td>\n      <td>-2.391483</td>\n      <td>-8.410171</td>\n      <td>-3.893566</td>\n      <td>-5.105922</td>\n      <td>-5.253990</td>\n      <td>2.073832</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0.000798</td>\n      <td>3.989460e-04</td>\n      <td>0.000598</td>\n      <td>0.000488</td>\n      <td>5</td>\n      <td>0</td>\n      <td>{'alpha': 5, 'l1_ratio': 0}</td>\n      <td>-22.172571</td>\n      <td>-15.069784</td>\n      <td>-22.993944</td>\n      <td>-19.282304</td>\n      <td>-16.044044</td>\n      <td>-19.112529</td>\n      <td>3.169147</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0.000798</td>\n      <td>3.988505e-04</td>\n      <td>0.000200</td>\n      <td>0.000399</td>\n      <td>5</td>\n      <td>0.25</td>\n      <td>{'alpha': 5, 'l1_ratio': 0.25}</td>\n      <td>-24.432713</td>\n      <td>-16.467589</td>\n      <td>-24.765866</td>\n      <td>-21.125351</td>\n      <td>-16.714276</td>\n      <td>-20.701159</td>\n      <td>3.590115</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0.000598</td>\n      <td>4.884999e-04</td>\n      <td>0.000546</td>\n      <td>0.000456</td>\n      <td>5</td>\n      <td>0.5</td>\n      <td>{'alpha': 5, 'l1_ratio': 0.5}</td>\n      <td>-27.793488</td>\n      <td>-18.602269</td>\n      <td>-27.107849</td>\n      <td>-23.945227</td>\n      <td>-18.064635</td>\n      <td>-23.102694</td>\n      <td>4.108297</td>\n      <td>18</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0.000599</td>\n      <td>4.888115e-04</td>\n      <td>0.000399</td>\n      <td>0.000489</td>\n      <td>5</td>\n      <td>0.75</td>\n      <td>{'alpha': 5, 'l1_ratio': 0.75}</td>\n      <td>-30.329236</td>\n      <td>-21.745310</td>\n      <td>-30.310118</td>\n      <td>-27.557643</td>\n      <td>-21.013430</td>\n      <td>-26.191148</td>\n      <td>4.062788</td>\n      <td>21</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0.000798</td>\n      <td>3.988029e-04</td>\n      <td>0.000199</td>\n      <td>0.000399</td>\n      <td>5</td>\n      <td>0.95</td>\n      <td>{'alpha': 5, 'l1_ratio': 0.95}</td>\n      <td>-31.130307</td>\n      <td>-22.549433</td>\n      <td>-31.155204</td>\n      <td>-27.963447</td>\n      <td>-21.698192</td>\n      <td>-26.899317</td>\n      <td>4.077240</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0.000997</td>\n      <td>2.861023e-07</td>\n      <td>0.000200</td>\n      <td>0.000399</td>\n      <td>5</td>\n      <td>0.99</td>\n      <td>{'alpha': 5, 'l1_ratio': 0.99}</td>\n      <td>-31.130307</td>\n      <td>-22.549433</td>\n      <td>-31.155204</td>\n      <td>-27.963447</td>\n      <td>-21.698192</td>\n      <td>-26.899317</td>\n      <td>4.077240</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>0.000798</td>\n      <td>3.989937e-04</td>\n      <td>0.000399</td>\n      <td>0.000488</td>\n      <td>5</td>\n      <td>1</td>\n      <td>{'alpha': 5, 'l1_ratio': 1}</td>\n      <td>-31.130307</td>\n      <td>-22.549433</td>\n      <td>-31.155204</td>\n      <td>-27.963447</td>\n      <td>-21.698192</td>\n      <td>-26.899317</td>\n      <td>4.077240</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>0.001197</td>\n      <td>3.986603e-04</td>\n      <td>0.000200</td>\n      <td>0.000399</td>\n      <td>10</td>\n      <td>0</td>\n      <td>{'alpha': 10, 'l1_ratio': 0}</td>\n      <td>-25.971218</td>\n      <td>-18.208243</td>\n      <td>-26.439339</td>\n      <td>-22.955705</td>\n      <td>-18.400074</td>\n      <td>-22.394916</td>\n      <td>3.548205</td>\n      <td>17</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>0.000798</td>\n      <td>3.989698e-04</td>\n      <td>0.000399</td>\n      <td>0.000489</td>\n      <td>10</td>\n      <td>0.25</td>\n      <td>{'alpha': 10, 'l1_ratio': 0.25}</td>\n      <td>-29.700286</td>\n      <td>-20.857661</td>\n      <td>-29.415492</td>\n      <td>-26.258753</td>\n      <td>-20.130788</td>\n      <td>-25.272596</td>\n      <td>4.090730</td>\n      <td>19</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>0.001090</td>\n      <td>1.852764e-04</td>\n      <td>0.000599</td>\n      <td>0.000798</td>\n      <td>10</td>\n      <td>0.5</td>\n      <td>{'alpha': 10, 'l1_ratio': 0.5}</td>\n      <td>-31.130307</td>\n      <td>-22.549433</td>\n      <td>-31.155204</td>\n      <td>-27.963447</td>\n      <td>-21.698192</td>\n      <td>-26.899317</td>\n      <td>4.077240</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>0.001197</td>\n      <td>3.992086e-04</td>\n      <td>0.000200</td>\n      <td>0.000399</td>\n      <td>10</td>\n      <td>0.75</td>\n      <td>{'alpha': 10, 'l1_ratio': 0.75}</td>\n      <td>-31.130307</td>\n      <td>-22.549433</td>\n      <td>-31.155204</td>\n      <td>-27.963447</td>\n      <td>-21.698192</td>\n      <td>-26.899317</td>\n      <td>4.077240</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>0.001390</td>\n      <td>4.817802e-04</td>\n      <td>0.000598</td>\n      <td>0.000489</td>\n      <td>10</td>\n      <td>0.95</td>\n      <td>{'alpha': 10, 'l1_ratio': 0.95}</td>\n      <td>-31.130307</td>\n      <td>-22.549433</td>\n      <td>-31.155204</td>\n      <td>-27.963447</td>\n      <td>-21.698192</td>\n      <td>-26.899317</td>\n      <td>4.077240</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>0.001396</td>\n      <td>4.883055e-04</td>\n      <td>0.000399</td>\n      <td>0.000489</td>\n      <td>10</td>\n      <td>0.99</td>\n      <td>{'alpha': 10, 'l1_ratio': 0.99}</td>\n      <td>-31.130307</td>\n      <td>-22.549433</td>\n      <td>-31.155204</td>\n      <td>-27.963447</td>\n      <td>-21.698192</td>\n      <td>-26.899317</td>\n      <td>4.077240</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>0.000798</td>\n      <td>3.989936e-04</td>\n      <td>0.000399</td>\n      <td>0.000489</td>\n      <td>10</td>\n      <td>1</td>\n      <td>{'alpha': 10, 'l1_ratio': 1}</td>\n      <td>-31.130307</td>\n      <td>-22.549433</td>\n      <td>-31.155204</td>\n      <td>-27.963447</td>\n      <td>-21.698192</td>\n      <td>-26.899317</td>\n      <td>4.077240</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>0.003391</td>\n      <td>2.053896e-03</td>\n      <td>0.000399</td>\n      <td>0.000488</td>\n      <td>50</td>\n      <td>0</td>\n      <td>{'alpha': 50, 'l1_ratio': 0}</td>\n      <td>-29.959161</td>\n      <td>-21.556223</td>\n      <td>-30.080478</td>\n      <td>-26.824729</td>\n      <td>-20.938967</td>\n      <td>-25.871912</td>\n      <td>3.956883</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>0.000997</td>\n      <td>8.920806e-07</td>\n      <td>0.000598</td>\n      <td>0.000489</td>\n      <td>50</td>\n      <td>0.25</td>\n      <td>{'alpha': 50, 'l1_ratio': 0.25}</td>\n      <td>-31.130307</td>\n      <td>-22.549433</td>\n      <td>-31.155204</td>\n      <td>-27.963447</td>\n      <td>-21.698192</td>\n      <td>-26.899317</td>\n      <td>4.077240</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>0.001196</td>\n      <td>7.459202e-04</td>\n      <td>0.000199</td>\n      <td>0.000399</td>\n      <td>50</td>\n      <td>0.5</td>\n      <td>{'alpha': 50, 'l1_ratio': 0.5}</td>\n      <td>-31.130307</td>\n      <td>-22.549433</td>\n      <td>-31.155204</td>\n      <td>-27.963447</td>\n      <td>-21.698192</td>\n      <td>-26.899317</td>\n      <td>4.077240</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>0.000599</td>\n      <td>4.887729e-04</td>\n      <td>0.000399</td>\n      <td>0.000488</td>\n      <td>50</td>\n      <td>0.75</td>\n      <td>{'alpha': 50, 'l1_ratio': 0.75}</td>\n      <td>-31.130307</td>\n      <td>-22.549433</td>\n      <td>-31.155204</td>\n      <td>-27.963447</td>\n      <td>-21.698192</td>\n      <td>-26.899317</td>\n      <td>4.077240</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>0.002593</td>\n      <td>1.196885e-03</td>\n      <td>0.000798</td>\n      <td>0.000746</td>\n      <td>50</td>\n      <td>0.95</td>\n      <td>{'alpha': 50, 'l1_ratio': 0.95}</td>\n      <td>-31.130307</td>\n      <td>-22.549433</td>\n      <td>-31.155204</td>\n      <td>-27.963447</td>\n      <td>-21.698192</td>\n      <td>-26.899317</td>\n      <td>4.077240</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>0.001197</td>\n      <td>3.988272e-04</td>\n      <td>0.000399</td>\n      <td>0.000488</td>\n      <td>50</td>\n      <td>0.99</td>\n      <td>{'alpha': 50, 'l1_ratio': 0.99}</td>\n      <td>-31.130307</td>\n      <td>-22.549433</td>\n      <td>-31.155204</td>\n      <td>-27.963447</td>\n      <td>-21.698192</td>\n      <td>-26.899317</td>\n      <td>4.077240</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>0.001197</td>\n      <td>3.990889e-04</td>\n      <td>0.000199</td>\n      <td>0.000399</td>\n      <td>50</td>\n      <td>1</td>\n      <td>{'alpha': 50, 'l1_ratio': 1}</td>\n      <td>-31.130307</td>\n      <td>-22.549433</td>\n      <td>-31.155204</td>\n      <td>-27.963447</td>\n      <td>-21.698192</td>\n      <td>-26.899317</td>\n      <td>4.077240</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>0.001397</td>\n      <td>4.884220e-04</td>\n      <td>0.000598</td>\n      <td>0.000488</td>\n      <td>100</td>\n      <td>0</td>\n      <td>{'alpha': 100, 'l1_ratio': 0}</td>\n      <td>-30.534802</td>\n      <td>-22.043843</td>\n      <td>-30.608400</td>\n      <td>-27.384285</td>\n      <td>-21.311346</td>\n      <td>-26.376535</td>\n      <td>4.016062</td>\n      <td>22</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>0.000840</td>\n      <td>3.154518e-04</td>\n      <td>0.000200</td>\n      <td>0.000399</td>\n      <td>100</td>\n      <td>0.25</td>\n      <td>{'alpha': 100, 'l1_ratio': 0.25}</td>\n      <td>-31.130307</td>\n      <td>-22.549433</td>\n      <td>-31.155204</td>\n      <td>-27.963447</td>\n      <td>-21.698192</td>\n      <td>-26.899317</td>\n      <td>4.077240</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>0.000998</td>\n      <td>3.568323e-07</td>\n      <td>0.000199</td>\n      <td>0.000399</td>\n      <td>100</td>\n      <td>0.5</td>\n      <td>{'alpha': 100, 'l1_ratio': 0.5}</td>\n      <td>-31.130307</td>\n      <td>-22.549433</td>\n      <td>-31.155204</td>\n      <td>-27.963447</td>\n      <td>-21.698192</td>\n      <td>-26.899317</td>\n      <td>4.077240</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>0.000598</td>\n      <td>4.884609e-04</td>\n      <td>0.000399</td>\n      <td>0.000489</td>\n      <td>100</td>\n      <td>0.75</td>\n      <td>{'alpha': 100, 'l1_ratio': 0.75}</td>\n      <td>-31.130307</td>\n      <td>-22.549433</td>\n      <td>-31.155204</td>\n      <td>-27.963447</td>\n      <td>-21.698192</td>\n      <td>-26.899317</td>\n      <td>4.077240</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>0.000798</td>\n      <td>3.988507e-04</td>\n      <td>0.000599</td>\n      <td>0.000799</td>\n      <td>100</td>\n      <td>0.95</td>\n      <td>{'alpha': 100, 'l1_ratio': 0.95}</td>\n      <td>-31.130307</td>\n      <td>-22.549433</td>\n      <td>-31.155204</td>\n      <td>-27.963447</td>\n      <td>-21.698192</td>\n      <td>-26.899317</td>\n      <td>4.077240</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>0.001247</td>\n      <td>9.719796e-04</td>\n      <td>0.000398</td>\n      <td>0.000488</td>\n      <td>100</td>\n      <td>0.99</td>\n      <td>{'alpha': 100, 'l1_ratio': 0.99}</td>\n      <td>-31.130307</td>\n      <td>-22.549433</td>\n      <td>-31.155204</td>\n      <td>-27.963447</td>\n      <td>-21.698192</td>\n      <td>-26.899317</td>\n      <td>4.077240</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>0.000886</td>\n      <td>4.758855e-04</td>\n      <td>0.000200</td>\n      <td>0.000399</td>\n      <td>100</td>\n      <td>1</td>\n      <td>{'alpha': 100, 'l1_ratio': 1}</td>\n      <td>-31.130307</td>\n      <td>-22.549433</td>\n      <td>-31.155204</td>\n      <td>-27.963447</td>\n      <td>-21.698192</td>\n      <td>-26.899317</td>\n      <td>4.077240</td>\n      <td>23</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(grid_model.cv_results_)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "2.387342642087474"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = grid_model.predict(x_test)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_test,y_pred)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
